{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN+LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyhCsFQ3S4J1EqN/SEqB1l",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonbrand/Gait-Recognition/blob/main/CNN%2BLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VKExr1xj1cd",
        "outputId": "47a0c5d4-bcf7-48ef-f2e9-e64b6f65a977"
      },
      "source": [
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8658607133437155166\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14512029696\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 9435736029789133901\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "td5cXpnWStoU"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hLOXsKVUHXF",
        "outputId": "a29a58c1-3b7c-4e6d-8357-d299bd68b846"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwh0ivOIUQGF"
      },
      "source": [
        "train_data_path='/content/drive/MyDrive/DL- Danny & Yonatan/Datasets for gait segmentation/Dataset #4/train/Inertial Signals'\n",
        "train_label_path='/content/drive/MyDrive/DL- Danny & Yonatan/Datasets for gait segmentation/Dataset #4/train/y_train.txt'\n",
        "\n",
        "test_data_path='/content/drive/MyDrive/DL- Danny & Yonatan/Datasets for gait segmentation/Dataset #4/test/Inertial Signals'\n",
        "test_label_path='/content/drive/MyDrive/DL- Danny & Yonatan/Datasets for gait segmentation/Dataset #4/test/y_test.txt'\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2QiHUQ2Smbg"
      },
      "source": [
        "#load data\n",
        "def load_X(path):\n",
        "    X_signals = []\n",
        "    files = os.listdir(path)\n",
        "    for my_file in files:\n",
        "        fileName = os.path.join(path,my_file)\n",
        "        file = open(fileName, 'r')\n",
        "        X_signals.append(\n",
        "            [np.array(cell, dtype=np.float32) for cell in [\n",
        "                row.strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "        #X_signals = 6*totalStepNum*128\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))#(totalStepNum*128*6)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHpVLpqCYstu"
      },
      "source": [
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    y_ = y_ - 1\n",
        "    #one_hot\n",
        "    y_ = y_.reshape(len(y_))\n",
        "    n_values = int(np.max(y_)) + 1\n",
        "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oUZUx1VTMIi"
      },
      "source": [
        "\n",
        "#---------------------------the part of CNN---------------------------------\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1) \n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "#----------------------------------the part of LSTM--------------------------------\n",
        "class Config(object):\n",
        "    \"\"\"\n",
        "    define a class to store parameters,\n",
        "    the input should be feature mat of training and testing\n",
        "    Note: it would be more interesting to use a HyperOpt search space:\n",
        "    https://github.com/hyperopt/hyperopt\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X_train, X_test):\n",
        "        # Input data\n",
        "        self.n_layers = 2   # nb of layers\n",
        "        self.train_count = len(X_train)  # 7352 training series\n",
        "        self.test_data_count = len(X_test)  # 2947 testing series\n",
        "        self.n_steps = len(X_train[0])  # 128 time_steps per series\n",
        "\n",
        "        # Training\n",
        "        self.learning_rate = 0.0025\n",
        "        self.lambda_loss_amount = 0.0015 #regularization\n",
        "        self.training_epochs = 300\n",
        "        self.batch_size = 1500\n",
        "\n",
        "        # LSTM structure\n",
        "        self.n_inputs = len(X_train[0][0]) #6  \n",
        "        self.n_hidden = 64  # nb of neurons inside the neural network\n",
        "        self.n_classes = 20  # Final output classes\n",
        "        self.W = {\n",
        "            'hidden': tf.Variable(tf.random.normal([self.n_inputs, self.n_hidden])),\n",
        "            'output': tf.Variable(tf.random.normal([self.n_hidden, self.n_classes]))\n",
        "        }\n",
        "        self.biases = {\n",
        "            'hidden': tf.Variable(tf.random.normal([self.n_hidden], mean=1.0)),\n",
        "            'output': tf.Variable(tf.random.normal([self.n_classes]))\n",
        "        }\n",
        "\n",
        "\n",
        "def LSTM_Network(_X, config):\n",
        "    \"\"\"Function returns a TensorFlow RNN with two stacked LSTM cells\n",
        "    Two LSTM cells are stacked which adds deepness to the neural network.\n",
        "    Note, some code of this notebook is inspired from an slightly different\n",
        "    RNN architecture used on another dataset, some of the credits goes to\n",
        "    \"aymericdamien\".\n",
        "    Args:\n",
        "        _X:     ndarray feature matrix, shape: [batch_size, time_steps, n_inputs]\n",
        "        config: Config for the neural network.\n",
        "    Returns:\n",
        "        This is a description of what is returned.\n",
        "    Raises:\n",
        "        KeyError: Raises an exception.\n",
        "      Args:\n",
        "        feature_mat: ndarray feature matrix, shape=[batch_size,time_steps,n_inputs]\n",
        "        config: class containing config of network\n",
        "      return:\n",
        "              : matrix  output shape [batch_size,n_classes]\n",
        "    \"\"\"\n",
        "    # (NOTE: This step could be greatly optimised by shaping the dataset once\n",
        "    # input shape: (batch_size, n_steps, n_input)\n",
        "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
        "    # Reshape to prepare input to hidden activation\n",
        "    _X = tf.reshape(_X, [-1, config.n_inputs])\n",
        "    # new shape: (n_steps*batch_size, n_input)\n",
        "\n",
        "    # Linear activation\n",
        "    _X = tf.nn.relu(tf.matmul(_X, config.W['hidden']) + config.biases['hidden'])\n",
        "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
        "    _X = tf.split(_X, config.n_steps, 0)\n",
        "    # new shape: n_steps * (batch_size, n_hidden)\n",
        "\n",
        "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
        "    lstm_cell_1 = tf.contrib.cudnn_rnn.CudnnLSTM(config.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cell_2 = tf.contrib.cudnn_rnn.CudnnLSTM(config.n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
        "    lstm_cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_cell_1, lstm_cell_2]*config.n_layers, state_is_tuple=True)\n",
        "    # Get LSTM cell output\n",
        "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
        "\n",
        "    # Get last time step's output feature for a \"many to one\" style classifier,\n",
        "    # as in the image describing RNNs at the top of this page\n",
        "    lstm_last_output = outputs[-1]\n",
        "\n",
        "    # Linear activation\n",
        "    #return tf.matmul(lstm_last_output, config.W['output']) + config.biases['output']\n",
        "    return  lstm_last_output\n",
        "\n",
        "def CNN_NetWork(X_):\n",
        "    CNN_input = tf.reshape(X_,[-1, 128, 6, 1])\n",
        "    # 1st conv layer & pooling, h_pool1(?*64*6*32)\n",
        "    W_conv1 = weight_variable([3, 3, 1, 32])  \n",
        "    b_conv1 = bias_variable([32])\n",
        "    h_conv1 = tf.nn.elu(\n",
        "        tf.nn.conv2d(CNN_input, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
        "    h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    # The second convolutional layer & pooling, (?*32*6*64 channels)\n",
        "    W_conv2 = weight_variable([3, 3, 32, 64])  # \n",
        "    b_conv2 = bias_variable([64])\n",
        "    h_conv2 = tf.nn.elu(\n",
        "        tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
        "    h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 1, 1], padding='SAME')\n",
        "\n",
        "    # The third layer is a fully connected layer, the input dimension is 32*3*64, and the output dimension is 64\n",
        "    W_fc1 = weight_variable([32 * 3 * 64, 64])\n",
        "    b_fc1 = bias_variable([64])\n",
        "    h_pool2_flat = tf.reshape(h_pool2, [-1, 32 * 3 * 64])\n",
        "    h_fc1 = tf.nn.elu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
        "    keep_prob = tf.placeholder(tf.float32) # Drop out is used here, which means that some cells are randomly arranged to have an output value of 0, which can prevent overfitting\n",
        "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
        "    return h_fc1_drop\n",
        "\n",
        "def last_full_connection_layer(lstm_output,cnn_output):\n",
        "    eigen_input = tf.concat([lstm_output, cnn_output],1)\n",
        "    # The fourth layer, input 64 dimensions, output 20 dimensions, which is the specific classification\n",
        "    W_fc2 = weight_variable([128, 20]) #20 is the number of participants to classify, it need to be changed in our case to 2 (PD/HC)\n",
        "    b_fc2 = bias_variable([20])\n",
        "    #y_conv = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)  \n",
        "    return tf.nn.softmax(tf.matmul(eigen_input, W_fc2) + b_fc2)  "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89iVs8lToCzF"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "X_ = tf.compat.v1.placeholder(tf.float32, [None, 128, 6])  # data\n",
        "label_ = tf.compat.v1.placeholder(tf.float32, [None, 20])  # label\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9LljBcEpTHC"
      },
      "source": [
        "#load data\n",
        "#input (num_of_samples, 128, 6)\n",
        "X_train = load_X(train_data_path) \n",
        "X_test = load_X(test_data_path)\n",
        "\n",
        "#label (num_of_samples, num_of_participants)\n",
        "train_label = load_y(train_label_path)\n",
        "test_label = load_y(test_label_path)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY9gz7sUmzV6"
      },
      "source": [
        "config = Config(X_train, X_test)\n",
        "lstm_output = LSTM_Network(X_,config)\n",
        "cnn_output = CNN_NetWork(X_)\n",
        "pred_Y = last_full_connection_layer(lstm_output,cnn_output)\n",
        "\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(label_ * tf.log(pred_Y+1e-10), reduction_indices=[1])) # loss function\n",
        "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy) #adam\n",
        "correct_prediction = tf.equal(tf.argmax(pred_Y,1), tf.argmax(label_,1)) # Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer()) # Variable initialization\n",
        "\n",
        "best_accuracy = 0\n",
        "for i in range(1):\n",
        "    batch_size = 512\n",
        "    for start,end in zip(range(0,len(train_label),batch_size),\n",
        "                         range(batch_size,len(train_label)+1,batch_size)):\n",
        "        sess.run(train_step,feed_dict={\n",
        "            X_:X_train[start:end],\n",
        "            label_:train_label[start:end]\n",
        "        })\n",
        "        # Test completely at every epoch: calculate accuracy\n",
        "    accuracy_out, loss_out = sess.run(\n",
        "        [accuracy, cross_entropy],\n",
        "        feed_dict={\n",
        "            X_:X_test,\n",
        "            label_:test_label\n",
        "        }\n",
        "    )\n",
        "    if accuracy_out > best_accuracy:\n",
        "        best_accuracy = accuracy_out\n",
        "    print(str(i)+'th cross_entropy:',str(loss_out),'accuracy:',str(accuracy_out))\n",
        "\n",
        "print(\"best accuracy:\"+str(best_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}